#pragma once

#include "core/concepts/GameConcept.hpp"

namespace core {

template <core::concepts::Game Game>
struct PolicyTarget {
  static constexpr const char* kName = "policy";
  using Tensor = Game::Types::PolicyTensor;

  template <typename GameLogView>
  static bool tensorize(const GameLogView& view, Tensor&);
};

// NOTE: If we add a game where we produce non-logit value predictions, we should modify this to
// allow customization. We will likely need this in single-player games.
template <core::concepts::Game Game>
struct ValueTarget {
  static constexpr const char* kName = "value";
  using Tensor = Game::Types::GameResultTensor;

  template <typename GameLogView>
  static bool tensorize(const GameLogView& view, Tensor&);
};

// ActionValueTarget is generated by evaluating the Value head on the states that result from taking
// each action from the current position.
template <core::concepts::Game Game>
struct ActionValueTarget {
  static constexpr const char* kName = "action_value";
  using Tensor = Game::Types::ActionValueTensor;

  template <typename GameLogView>
  static bool tensorize(const GameLogView& view, Tensor&);
};

// QPosteriorTarget is used to train the ValueUncertainty head, which predicts the squared
// difference between Q prior and Q posterior.
//
// We could compute this squared difference directly and export it as a target, but we instead
// follow the example of KataGo and export Q posterior itself as a target, and let the
// ValueUncertainty head compute the squared difference based on the output of the Value head
// (which represents the Q prior).
template <core::concepts::Game Game>
struct QPosteriorTarget {
  static constexpr const char* kName = "Q_posterior";
  using Tensor = Game::Types::WinShareTensor;

  template <typename GameLogView>
  static bool tensorize(const GameLogView& view, Tensor&);
};

// QMinTarget is used to train the ValueUncertainty head.
template <core::concepts::Game Game>
struct QMinTarget {
  static constexpr const char* kName = "Q_min";
  using Tensor = Game::Types::WinShareTensor;

  template <typename GameLogView>
  static bool tensorize(const GameLogView& view, Tensor&);
};

// QMaxTarget is used to train the ValueUncertainty head.
template <core::concepts::Game Game>
struct QMaxTarget {
  static constexpr const char* kName = "Q_max";
  using Tensor = Game::Types::WinShareTensor;

  template <typename GameLogView>
  static bool tensorize(const GameLogView& view, Tensor&);
};

// WMaxTarget is used to train the ValueUncertainty head.
template <core::concepts::Game Game>
struct WMaxTarget {
  static constexpr const char* kName = "W_max";
  using Tensor = Game::Types::WinShareTensor;

  template <typename GameLogView>
  static bool tensorize(const GameLogView& view, Tensor&);
};

// ActionValueUncertaintyTarget is generated by evaluating the ValueUncertainty head on the states
// that result from taking each action from the current position.
template <core::concepts::Game Game>
struct ActionValueUncertaintyTarget {
  static constexpr const char* kName = "action_value_uncertainty";
  using Tensor = Game::Types::ActionValueTensor;

  template <typename GameLogView>
  static bool tensorize(const GameLogView& view, Tensor&);
};

template <core::concepts::Game Game>
struct ValidActionsTarget {
  static constexpr const char* kName = "valid_actions";
  using Tensor = Game::Types::PolicyTensor;

  template <typename GameLogView>
  static bool tensorize(const GameLogView& view, Tensor&);
};

template <core::concepts::Game Game>
struct OppPolicyTarget {
  static constexpr const char* kName = "opp_policy";
  using Tensor = Game::Types::PolicyTensor;

  template <typename GameLogView>
  static bool tensorize(const GameLogView& view, Tensor&);
};

namespace alpha0 {

template <core::concepts::Game Game>
struct StandardTrainingTargets {
  using PolicyTarget = core::PolicyTarget<Game>;
  using ValueTarget = core::ValueTarget<Game>;
  using ActionValueTarget = core::ActionValueTarget<Game>;
  using ValidActionsTarget = core::ValidActionsTarget<Game>;
  using OppPolicyTarget = core::OppPolicyTarget<Game>;

  using List =
    mp::TypeList<PolicyTarget, ValueTarget, ActionValueTarget, ValidActionsTarget, OppPolicyTarget>;
};

template <core::concepts::Game Game>
using StandardTrainingTargetsList = typename StandardTrainingTargets<Game>::List;

}  // namespace alpha0

namespace beta0 {

template <core::concepts::Game Game>
struct StandardTrainingTargets {
  using QPosteriorTarget = core::QPosteriorTarget<Game>;
  using QMinTarget = core::QMinTarget<Game>;
  using QMaxTarget = core::QMaxTarget<Game>;
  using WMaxTarget = core::WMaxTarget<Game>;
  using ActionValueUncertaintyTarget = core::ActionValueUncertaintyTarget<Game>;

  using List1 = alpha0::StandardTrainingTargets<Game>::List;
  using List2 = mp::TypeList<QPosteriorTarget, QMinTarget, QMaxTarget, WMaxTarget,
                             ActionValueUncertaintyTarget>;
  using List = mp::Concat_t<List1, List2>;
};

template <core::concepts::Game Game>
using StandardTrainingTargetsList = typename StandardTrainingTargets<Game>::List;

}  // namespace beta0

}  // namespace core

#include "inline/core/TrainingTargets.inl"
