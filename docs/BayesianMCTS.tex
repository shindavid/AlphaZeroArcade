\documentclass[tikz]{article}
\usepackage{tikz}
\usetikzlibrary{trees, arrows}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{bytefield, amsfonts, amsmath}
\usepackage{tcolorbox}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{appendix}

\setlength\parindent{0pt}

\newcommand{\Mod}[1]{\ (\mathrm{mod}\ #1)}
\newcommand{\argmax}{\mathrm{argmax}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\logit}{\mathrm{logit}}
\newcommand{\cross}{\times}

\graphicspath{ {./images/} }

\newtheorem{theorem}{Theorem}

\title{Bayesian MCTS}
\author{David Shin}
\date{February 24, 2025}

\begin{document}

\maketitle

\section{Introduction}

We propose a new variant of MCTS, which we call \textit{Bayesian MCTS}. Bayesian MCTS aims to address shortcomings of standard MCTS
by infusing it with Bayesian mechanics.

\section{Background: Grill et al}

Grill et al\footnote{\url{https://arxiv.org/abs/2007.12509}} provided an interpretation of the mechanics of MCTS: at each node, 
we maintain a policy. That policy is initialized with a prior, and repeatedly sharpened based on evidence observed from its descendants. \newline

It is instructive to rigorously describe MCTS mechanics in terms of this interpretation. We do so using Python syntax:

\begin{tcolorbox}
\begin{verbatim}
Policy = np.ndarray  # array of floats

@dataclass
class Stats:
    P: Policy   # prior policy, comes from neural net
    pi: Policy  # posterior policy, initialized to self.P
    Q: float    # quality estimate
    N: int      # visit count

@dataclass
class Node:
    children: List[Node]
    stats: Stats
\end{verbatim}
\end{tcolorbox}

In standard MCTS implementations, we don't maintain \texttt{pi} separately from \texttt{N}, but we do so here for pedantic purposes. \newline

At each step, we start with a parent node, and visit one of its children. The visit of the child produces evidence, 
which can be summarized as a pair of \texttt{Stats}: the child stats before the visit, 
and the child stats after the visit. The evidence is used to update the parent stats. The update function looks like this:

\begin{tcolorbox}
\begin{verbatim}
class Node:
    ...
    def update(self, child_index: int, before: Stats, after: Stats):
        self.update_posterior(child_index, before, after)
        self.stats.N += 1
        self.stats.Q = sum(p*c.stats.Q for p, c in zip(self.stats.pi, self.children))
\end{verbatim}
\end{tcolorbox}

The last line in the above is a natural computation to perform in a multi-armed bandit setting: 
to calculate the expected payout from pulling the lever of a randomly selected slot machine, 
we compute each slot machine's expected payout, and take their average, weighted by the probability of
selecting each slot machine\footnote{Here, we always let \texttt{Q} be from a fixed player's point-of-view, taking on values in the range $[0, 1]$, for simplicity of exposition}. \newline

Filling in the posterior-update method completes the description:

\begin{tcolorbox}
\begin{verbatim}
class Node:
    ...
    def update_posterior(self, child_index: int, before: Stats, after: Stats):
        if self.stats.N == 0:
            self.stats.pi *= 0
            self.stats.pi[child_index] += 1
        else:
            self.stats.pi[child_index] += 1 / self.stats.N

        self.stats.pi /= sum(self.stats.pi)  # normalize
\end{verbatim}
\end{tcolorbox}

Again, standard MCTS implementations do not maintain \texttt{pi} separately from \texttt{N}, so they would not include
code that looks like the above. When the logic is expressed purely in terms of \texttt{N}, it looks more natural. When \texttt{pi}
is forcibly extracted out, we are faced with the reality of this very unnatural-looking posterior-update function!
It begs many questions:

\begin{itemize}
    \item When \texttt{self.stats.N == 0}, why do we zero out the prior and set the posterior to a singular distribution\footnote{
    There is in fact a technical reason here: without zeroing out, we would need \texttt{Q} values on unvisited children when recomputing
    the parent \texttt{Q}, which we don't have in standard AlphaZero. In \href{https://github.com/shindavid/AlphaZeroArcade}{AlphaZeroArcade},
    we introduced an \emph{action-value} head (\texttt{AV}) that predicts the network's own \texttt{V}-prediction of the children - this invention
    allows us to avoid this zero-out.}?
    Surely, that choice is not the one that optimally combines the prior with the observed data?
    \item If \texttt{before.Q > after.Q}, that means that our belief of the quality of the visited child decreased. Why then, do we increase the policy weight for this child?
    \item The \texttt{after} evidence might show us that a child is provably winning. Why not incorporate such evidence by collapsing the posterior?
\end{itemize}

Of course, the answer here is that MCTS was not \textit{designed} with the Grill et al interpretation in mind. Rather, it \textit{evolved} into
its present form, and the Grill et al interpretation was \textit{discovered} after the fact. \newline

This begs the question: can we do better if we use the Grill et al interpretation as our starting point, and if we design our posterior-policy update
function from first principles?

\section{A Better Posterior Policy Update}

\subsection{Preliminaries}

In order to design a better posterior-policy update function, for usage in Bayesian MCTS, let us lay a stronger foundation. \newline

The parent node $p$ has $n$ children, $c_1, c_2, \ldots, c_n$. For each child, $c_i$, we have a corresponding
quality estimate, $Q_i$.
Although it is a scalar, it actually represents the mean of a \emph{distribution}, $D_i$, that represents our belief of the
true quality of $c_i$\footnote{The word \emph{true} might be slightly confusing here. For a game like chess or go, the game will
end in either a win or a loss (or a draw) - what does it mean for the ``true'' quality of a game state to be a fractional value like 0.36?
I have some ideas on how to formalize this, but it's kind of just a philosophical quibble.}.
In turn, each $D_i$ represents a \emph{projection} of some \emph{joint} distribution, $J$, expressible as
a probability distribution over $\mathbb{R}^n$. The true quality of the $n$ children is expressible as a point $x \in \mathbb{R}^n$,
and this implicit joint distribution $J$ represents our beliefs about $x$.\newline

Finally, the policy $\pi$ is a prediction of the coordinate along which $x$ is maximal. In other words,

$$
\pi_k = \Pr_{x \sim J}[k = \argmax_i\ x_i]
$$

Figure \ref{Q-figure} shows how one might visualize these distributions and scalars for a parent node $p$ that has two children. 

\begin{figure}[h]
\caption{Possible illustration of the beliefs of a 2-child parent node.}
\label{Q-figure}
\centering
\includegraphics[scale=0.5]{Q}
\end{figure}

Note:

\begin{itemize}
    \item The $x$-axis corresponds to child $c_x$, and the $y$-axis corresponds to child $c_y$.
    \item The blue dots represent the joint distribution $J$.
    \item The marginal distributions $D_x$ and $D_y$ obtained by projecting $J$ on the $x$-axis and $y$-axis, respectively, are plotted above and to the right.
    \item The mean of $D_x$ is $0.4$, and the mean of $D_y$ is $0.5$.
    \item $80\%$ of the blue dots have $x<y$, and $20\%$ of the blue dots have $x>y$.
\end{itemize}

Thus, this distribution $J$ is consistent with the following beliefs:

\begin{align*}
Q_1 &= 0.4 \\
Q_2 &= 0.5 \\
\pi &= [0.2, 0.8]
\end{align*}

Of course, this particular $J$ is not the \emph{only} distribution satisfying those $Q$ and $\pi$ constraints. It is merely one of
many possible such distributions. \newline

The goal of our posterior policy update function is to \textbf{recompute $\pi$, given new information about $D_i$}.

\subsection{Uncertainty Head}

If our new information about $D_i$ is restricted to merely an updated mean, our challenge becomes apparent. There are many ways that the joint 
distribution $J$ could change to be consistent with this new information. These different possibilities correspond to different
resultant $\pi$ distributions. \newline

Incorporating other statistical properties of the $D_i$ distributions would reduce our degrees of freedom, which could potentially
make our search for a viable posterior-policy update-rule easier. To this end, we propose introducing a new output head, the \emph{uncertainty head} (\texttt{U}),
that aims to predict how certain the network is in its \texttt{V} prediction. KataGo already has a head that predicts uncertainty\footnote{See \url{https://github.com/lightvector/KataGo/blob/master/docs/KataGoMethods.md\#uncertainty-weighted-mcts-playouts}}; our \texttt{U} head will
be based on KataGo. \newline

Our goal is for the \texttt{U} head output at child $c_i$ to correspond to the variance of marginal distribution $D_i$. If we can have
that starting point, then we can dynamically update this variance belief at each node by crafting an appropriate update-rule, analogous
to the way that we update $Q$ at a parent based on the $Q$ of its children. \newline

A bit of notational clarification is in order. For marginal distribution means, we have two different variables: $V$ represents a static prediction
from a neural network, and $Q$ represents a dynamically updated estimate based on those $V$ predictions. Similarly, for marginal distribution
variances, we will have two different variables: $U$ will represent a static prediction from a neural network, and $W$ will represent
a dynamically updated estimate based on those $U$ predictions. \newline

We start by deriving the $W$ update-rule. From standard properties of variance, we know that a random variable $X$ satisfies,

\begin{equation}
\label{variance}    
\Var[X] = \mathbb{E}[X^2] - \mathbb{E}[X]^2
\end{equation}

Let $D$ be the parent quality-belief distribution, with mean $Q$ and variance $W$, and let $D_i$ be the quality-belief distribution of
child $c_i$, with mean $Q_i$ and variance $W_i$. Applying (\ref{variance}) twice, we can derive our $W$ update rule\footnote{
As noted before, AlphaZeroArcade has an \emph{action-value} head (\texttt{AV}) that provides us with $Q$ values for unexpanded children.
In order to use this $W$ update rule, we similarly require an \emph{action-uncertainty} head (\texttt{AU}), which can similarly be
trained to predict the output of the network's own $U$-prediction of the children.}:

\begin{equation}
\label{W-update}    
W = \sum_{i} \pi_i (W_i + Q_i^2) - Q^2
\end{equation}

It remains to come up with a way to create training targets for the $U$ head. Again, taking inspiration from KataGo, we propose
a training target for the $U$ head of:

$$\hat{U} = (V - Q^*)^2,$$

where

$$Q^* = \lambda Q_{post} + (1-\lambda)z$$

for some constant $\lambda$. Here:

\begin{itemize}
    \item $V$ is the initial quality estimate of the root, which comes from the neural network
    \item $Q_{post}$ is the estimate of $Q$ at the root \emph{after} all MCTS iterations have been performed.
    \item $z$ is the final game result
\end{itemize}
This proposal is justified because variance is defined as the expected squared difference between a random variable and its mean.
Our prediction $V$ is the mean, and $Q^*$ corresponds to a sample from the corresponding random variable. We define this $Q^*$
as a blend of a short-term refinement that comes from MCTS and a ground-truth value that comes from the final game result.
My feeling is that blending in the final game result is necessary, especially in generation-0, since AlphaZeroArcade works by
substituting a ``dummy'' neural network that produces uniform predictions, for which we will typically have $V = Q_{post}$.
Without the $z$ term, the $\hat{U}$ targets would all be 0, which would cause the \texttt{U}-head to collapse towards 0. \newline

Some experimentation is warranted here. 

\subsection{Posterior Policy Update Rule}\label{posterior-section}

Let us turn our attention back to devising a posterior policy update rule. \newline

To tackle this, let us start with a simpler case. Suppose that there are only two children, and that the
distributions $D_1$ and $D_2$ are independent \textit{logit-normal} distributions of mean/variance $(\mu_1, \sigma_1^2)$ and $(\mu_2, \sigma_2^2)$, respectively.
By \textit{logit-normal}, we mean that the logit of the random variable is normally distributed. In other words, for some $\theta_i$ and $\omega_i$, we have:

\begin{align*}
\mathbb{E}_{x \sim D_i}[x] &= \mu_i \\
\mathbb{E}_{x \sim D_i}[(x - \mu_i)^2] &= \sigma_i^2 \\
\logit(D_i) &\sim \mathcal{N}(\theta_i, \omega_i^2)
\end{align*}

Given these idealized assumptions of independence and logit-normality, we can compute a good approximation to $\pi$. That
approximation is $\pi = [p, 1-p]$, where:

$$
p = \Phi\left(\frac{\theta_1 - \theta_2}{\sqrt{\omega_1^2 + \omega_2^2}}\right),
$$

where

\begin{eqnarray*}
\theta_i &=& \ln{\frac{\mu_i}{1-\mu_i}} - \frac{(1 - 2\mu_i)\sigma_i^2}{2\mu_i^2(1-\mu_i)^2} \\
\omega_i^2 &=& \frac{\sigma_i^2}{\mu_i^2(1-\mu_i)^2}
\end{eqnarray*}

(See \ref{appendix-derivations} for the derivation.) \newline

Suppose that we receive an information update of the form $(\mu_1, \sigma_1) \mapsto (\mu'_1, \sigma'_1)$.
This would update our policy to $\pi = [q, 1-q]$, where:

$$
q = \Phi\left(\frac{\theta'_1 - \theta_2}{\sqrt{\omega'_1^2 + \omega_2^2}}\right),
$$

with $\theta'_1$ and $\omega'_1$ defined analogously. \newline

Equivalently, we could frame this update rule as multiplying $\pi(1)$ by $\alpha$ and then normalizing $\pi$, where

$$
\alpha = \frac{q(1-p)}{p(1-q)}
$$

Can we generalize this approach? Unfortunately, when we generalize beyond 2-dimensions, I believe the problem has no known
analytical solution\footnote{According to \url{https://mathoverflow.net/q/153039}, there is no known analytical solution for an arbitrary $n$-dimensional
Gaussian $\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$. Our problem corresponds to the special case when $\boldsymbol{\Sigma}$ is a diagonal matrix. I \emph{believe}
this special case also has no known analytical solution.}. However, perhaps we can simply perform this computation
independently against each sibling and combine the results in some way? \newline

One idea is to compute the $\alpha$ term against each sibling, and then multiply by the geometric mean of all of them, followed by a normalization.
Experimentation is needed. \newline

It is worth discussing \emph{why}, even in the 2-child case, we multiply by $\alpha$ rather than simply rewriting $\pi$ to $[q, 1-q]$.
Given specific beliefs of $(\mu_1, \mu_2, \sigma_1, \sigma_2)$, we have derived above what our policy belief should be, \textit{if we make
the idealized assumption that the child distributions are independent and logit-normal}. For clarity, let us call this hypothetical policy $\pi^*$.
In actuality, this idealized assumption may not hold, and for that reason, we have some \textit{actual} policy belief, $\pi$, that in general might not
equal $\pi^*$. In some sense, the difference between $\pi$ and $\pi^*$ \emph{encodes} our belief, $B$, regarding the gap between
the idealized assumption and reality. \newline

When we incorporate the information update of $(\mu_1, \sigma_1) \mapsto (\mu'_1, \sigma'_1)$, we want to do so in
a manner that is appropriately faithful to this belief $B$. If we simply rewrote $\pi$ to $[q, 1-q]$, that would be completely
ignoring $B$, which is inappropriate. We therefore instead compute the \textit{multiplicative transform} ($\alpha$)
that \textit{would} correctly transform $\pi^*$ under the idealized assumption. \textit{Then}, we apply that same
multiplicative transform $\alpha$ to our \textit{actual} $\pi$, in hopes that doing so will be sufficiently faithful to that
belief $B$.

\subsection{Terminal State Updates}

One of the motivations of introducing a custom posterior policy update function was to better incorporate knowledge from terminal states. \newline

A terminal state is characterized as having zero uncertainty, which in our formulation amounts to a node with $(Q, U) = (1, 0)$. For
these values, we will find that $q=1$, and thus that $\alpha = +\infty$. Applying the Bayesian update and
renormalizing will collapse $\pi$ to have all its weight on the winning action, as desired. Similarly, a losing terminal state maps
to $(Q, U) = (0, 0)$, and for this, $\pi$ will collapse to zero out the losing action. The collapse will also immediately propagate up the tree via
mini-max-like mechanics. This represents a significant improvement over the slow-moving, gradually updating characterized by vanilla MCTS. \newline

\subsection{Summary}

To summarize, in Bayesian MCTS, we will maintain an additional dynamic value at each node: $W$, measuring the uncertainty in $Q$.
Its initial value will come from the output of the network's \texttt{U} head.
Whenever a child's stats are updated, we can update the parent's stats by the following update rules sequentially:

\begin{itemize}
    \item \textbf{$\pi$ update rule}: (as described in Section \ref{posterior-section})
    \item \textbf{$Q$ update rule}: $Q = \sum_i \pi_i Q_i$
    \item \textbf{$W$ update rule}: $W = \sum_i \pi_i(W_i+Q_i^2) - Q^2$
\end{itemize}

$Q$ and $W$ values for unexpanded children will come from the parent's network evaluation (the network's \texttt{AV} and \texttt{AU} heads).

\newpage 

\section{Replacing the PUCT Criterion}

So far, we have only discussed one component in MCTS behavior: the updating of the posterior policy. Of course, there is another
important component: \textbf{the decision of which leaf node to expand at each MCTS iteration}. \newline

Standard MCTS uses the PUCT criterion for this, a choice that is theoretically justified by the analysis of Grill et al. Despite the
theoretical justification, some exploratory noise is required in practice because the number of iterations ($n$) is finite. Dirichlet
noise is used for this purpose. \newline

In Bayesian MCTS, we have much more freedom in how to apply exploratory noise. We could, for example, simply randomly choose a child $c$ of
the root and make $k$ initial visits to $c$. Because we have decoupled $N$ from $\pi$, this should in principle not systematically bias $\pi$. We can
experiment with various choices, but I expect that some variant of this may be sufficient.\newline

Regarding replacing PUCT, my intuition is that we can reframe action selection in terms of the objective of \textbf{minimizing uncertainty at the root}.
That is, at each MCTS iteration, identify the leaf whose expansion would produce the \textit{largest expected decrease} in uncertainty ($W$) at the root. \newline

Let us derive a replacement action selection criterion based on this idea. This will result in something
computationally intractable, but the derivation will prove useful for designing something computationally feasible. \newline

Expanding a leaf, $\ell$, will update its $(Q_\ell, W_\ell)$ values, 
from initial values coming from the network's \texttt{AV} and \texttt{AU} heads, to new values. 
This corresponds to an update $\Delta (Q_\ell, W_\ell)$.
If we have a model $\mathcal{M}_\ell$ of the \textit{expected distribution} of this update,
then we can, in principle, analytically derive a distribution of the update $\Delta(Q_p, W_p)$ that would be incurred by the leaf's parent $p$ 
if we sample $\Delta (Q_\ell, W_\ell)$ from $\mathcal{M}_\ell$ and then apply the $Q$, $W$, and $\pi$ update rules.
This analytical derivation can be recursively performed all the way up to the root of the tree. For the base
case of this recursion, we need to supply a leaf model $\mathcal{M}_\ell$. For $\Delta Q$, we can
use a logit-normal distribution parameterized by $W$. For $\Delta W$, I suspect a simple fixed-shape distribution
should suffice; if not, we may need to resort to another network head that predicts the uncertainty of $U$. \newline

That would be the theoretically principled approach. Let us devise something practical. Here is our proposal. At the newly
expanded leaf, $\ell$, after initializing its stats from the network output, we estimate the distribution of $\Delta(Q_\ell, W_\ell)$
updates that $\ell$ would undergo \textbf{on the next visit to $\ell$}. This distribution can be summarized by a mean and variance.
As we traverse up the tree, we do this at each node $n$: after updating $n$'s stats via the update rules, we estimate the
distribution of $\Delta(Q_n, W_n)$ updates that $n$ would undergo on the next visit to $n$. This can be computed
based on the update distributions at each of $n$'s children, which we will have precomputed for all children from
previous visits. \newline

This provides us with a per-node annotation of \textit{``how much are my $(Q, W)$ values expected to change the next time I am visited?''}.
This in turns gives us potential for an action selection criterion. Starting at the root, $r$, we can estimate how much $(Q_r, W_r)$
will change by visiting child $c$, for each child $c$, by using the annotation at $c$, combined with the $\pi$, $Q$, and $W$
update rules. We can repeat this to crawl all the way down to a leaf. \newline

Obviously, there are a lot of details to flush out here, but I think this is a viable high-level plan.


\newpage
\appendix
\renewcommand{\thesection}{Appendix \Alph{section}} % Format for appendix section titles

\section{Derivations}
\label{appendix-derivations}

Here we derive the policy approximation formula in the two-child case under the idealized assumption of independence and logit-normality. \newline

Suppose $X$ is a random variable with mean $\mu$ and variance $\sigma^2$. Let us derive the mean and variance of $Y = f(X)$, where

$$f(x) = \logit(x) = \ln{\frac{x}{1-x}}$$

Note that:

\begin{align*}
f'(x) &= \frac{1}{x(1-x)} \\
f''(x) &= -\frac{1-2x}{x^2(1-x)^2}
\end{align*}

If we assume that $\sigma^2$ is small and $X \approx \mu$, we can use a second-order Taylor approximation:

$$Y = f(X) \approx f(\mu) + f'(\mu)(X-\mu) + \frac{1}{2}f''(\mu)(X-\mu)^2.$$

Taking an expectation of both sides, we get:

$$
\mathbb{E}[Y] \approx f(\mu) + \frac{1}{2} f''(\mu)\sigma^2 = \ln{\frac{\mu}{1-\mu}} - \frac{(1-2\mu)\sigma^2}{2\mu^2(1-\mu)^2}
$$

Similarly, we can derive,

$$
\mathrm{Var}(Y) = \mathbb{E}[(Y - \mathbb{E}[Y])^2] \approx f'(u)^2\sigma^2 = \frac{\sigma^2}{\mu^2(1-\mu)^2}
$$

Thus, if $Y$ is indeed normally distributed as $\mathcal{N}(\theta, \omega^2)$, then we must have:

\begin{align*}
\theta &= \ln{\frac{\mu}{1-\mu}} - \frac{(1-2\mu)\sigma^2}{2\mu^2(1-\mu)^2} \\
\omega^2 &= \frac{\sigma^2}{\mu^2(1-\mu)^2},
\end{align*}
as claimed.

\end{document}
