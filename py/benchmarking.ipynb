{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5c1fdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphazero.logic.evaluator import Evaluator\n",
    "from alphazero.logic.benchmarker import Benchmarker\n",
    "from alphazero.logic.run_params import RunParams\n",
    "from alphazero.servers.loop_control.directory_organizer import DirectoryOrganizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=1, suppress=True, linewidth=500, threshold=np.inf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b557931-1c2d-47c4-bd3d-d642bdcf5ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = 'c4'\n",
    "benchmark_tag = 'benchmark'\n",
    "run_params = RunParams(game, benchmark_tag)\n",
    "organizer = DirectoryOrganizer(run_params)\n",
    "benchmark_rating_data = evaluator._benchmark.read_ratings_from_db()\n",
    "benchmark_gens = np.array([agent.gen for agent in benchmark_rating_data.agents])\n",
    "benchmark_ratings = benchmark_rating_data.ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86acfd6e-42c4-4e4f-9d96-9802102f0054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkRatingData(agents=[], ratings=array([], dtype=float64), committee=array([], dtype=bool))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_rating_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e856e93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = 'c4'\n",
    "tag = 'eval'\n",
    "run_params = RunParams(game, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bb1360b-d700-4bc4-a487-025100324355",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "unable to open database file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m organizer \u001b[38;5;241m=\u001b[39m DirectoryOrganizer(run_params, base_dir_root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/workspace\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m \u001b[43mEvaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43morganizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m benchmark_rating_data \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39m_benchmark\u001b[38;5;241m.\u001b[39mread_ratings_from_db()\n\u001b[1;32m      4\u001b[0m benchmark_gens \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([agent\u001b[38;5;241m.\u001b[39mgen \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m benchmark_rating_data\u001b[38;5;241m.\u001b[39magents])\n",
      "File \u001b[0;32m/workspace/repo/py/alphazero/logic/evaluator.py:29\u001b[0m, in \u001b[0;36mEvaluator.__init__\u001b[0;34m(self, organizer)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, organizer: DirectoryOrganizer):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_organizer \u001b[38;5;241m=\u001b[39m organizer\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_benchmark \u001b[38;5;241m=\u001b[39m \u001b[43mBenchmarker\u001b[49m\u001b[43m(\u001b[49m\u001b[43morganizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morganizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_db_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_benchmark_rating_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_benchmark\u001b[38;5;241m.\u001b[39mread_ratings_from_db()\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arena \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_benchmark\u001b[38;5;241m.\u001b[39mclone_arena()\n",
      "File \u001b[0;32m/workspace/repo/py/alphazero/logic/benchmarker.py:43\u001b[0m, in \u001b[0;36mBenchmarker.__init__\u001b[0;34m(self, organizer, db_filename)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_db \u001b[38;5;241m=\u001b[39m RatingDB(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_organizer\u001b[38;5;241m.\u001b[39mbenchmark_db_filename)\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/repo/py/alphazero/logic/benchmarker.py:46\u001b[0m, in \u001b[0;36mBenchmarker.load_from_db\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_from_db\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arena\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_agents_from_db\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_db\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrole\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAgentRole\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBENCHMARK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arena\u001b[38;5;241m.\u001b[39mload_matches_from_db(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_db, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mMatchType\u001b[38;5;241m.\u001b[39mBENCHMARK)\n",
      "File \u001b[0;32m/workspace/repo/py/alphazero/logic/arena.py:33\u001b[0m, in \u001b[0;36mArena.load_agents_from_db\u001b[0;34m(self, db, role)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_agents_from_db\u001b[39m(\u001b[38;5;28mself\u001b[39m, db: RatingDB, role: Optional[AgentRole]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m db_agent \u001b[38;5;129;01min\u001b[39;00m db\u001b[38;5;241m.\u001b[39mfetch_agents():\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m role \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m db_agent\u001b[38;5;241m.\u001b[39mrole \u001b[38;5;241m!=\u001b[39m role:\n\u001b[1;32m     35\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/repo/py/alphazero/logic/rating_db.py:49\u001b[0m, in \u001b[0;36mRatingDB.fetch_agents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfetch_agents\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterable[DBAgent]:\n\u001b[1;32m     43\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    Constructs a DBAgent from each row of the agents table, and returns a list of them.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m    TODO: potentially consider adding an optional argument to specify a minimum agent id to\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m    fetch. Callers could use this to fetch only agents added since the last fetch.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdb_conn_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     c \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[1;32m     52\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\u001b[38;5;124mSELECT agents.id, gen, n_iters, tag, is_zero_temp, role\u001b[39m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124m               FROM agents\u001b[39m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124m               JOIN mcts_agents\u001b[39m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124m               ON agents.sub_id = mcts_agents.id\u001b[39m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124m               WHERE subtype=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmcts\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124m               \u001b[39m\u001b[38;5;124m'''\u001b[39m\n",
      "File \u001b[0;32m/workspace/repo/py/util/sqlite3_util.py:72\u001b[0m, in \u001b[0;36mDatabaseConnectionPool.get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn_dict) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     70\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating new connection: db_filename=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m thread=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     71\u001b[0m                  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_db_filename, thread_name, n)\n\u001b[0;32m---> 72\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn_dict[thread_id] \u001b[38;5;241m=\u001b[39m conn\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
      "File \u001b[0;32m/workspace/repo/py/util/sqlite3_util.py:85\u001b[0m, in \u001b[0;36mDatabaseConnectionPool._create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_cmds:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatabase file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_db_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist and create_cmds is not specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m create_conn \u001b[38;5;241m=\u001b[39m \u001b[43msqlite3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_db_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m cursor \u001b[38;5;241m=\u001b[39m create_conn\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cmd \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_cmds:\n",
      "\u001b[0;31mOperationalError\u001b[0m: unable to open database file"
     ]
    }
   ],
   "source": [
    "organizer = DirectoryOrganizer(run_params, base_dir_root='/workspace')\n",
    "evaluator = Evaluator(organizer)\n",
    "benchmark_rating_data = evaluator._benchmark.read_ratings_from_db()\n",
    "benchmark_gens = np.array([agent.gen for agent in benchmark_rating_data.agents])\n",
    "benchmark_ratings = benchmark_rating_data.ratings\n",
    "committee_gens = [agent.gen for i, agent in zip(benchmark_rating_data.committee, benchmark_rating_data.agents) if i == True]\n",
    "eval_rating_data = evaluator.read_ratings_from_db()\n",
    "evaluated_gens = np.array([agent.gen for agent in eval_rating_data.evaluated_agents])\n",
    "evaluated_ratings = eval_rating_data.ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfc1734-b6d3-4b82-bf1a-c0c041307867",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(16,8))\n",
    "\n",
    "sorted_ix = np.argsort(benchmark_gens)\n",
    "\n",
    "axes.vlines(benchmark_gens, ymin=min(benchmark_ratings), ymax=max(benchmark_ratings)*1.2, color='lightgrey', linestyle='--', label='participated agents')\n",
    "axes.vlines(committee_gens, ymin=min(benchmark_ratings), ymax=max(benchmark_ratings)*1.2, color='orange', linestyle='--', label='committee')\n",
    "axes.plot(benchmark_gens[sorted_ix], benchmark_ratings[sorted_ix], label='elo', zorder=4)\n",
    "\n",
    "axes.scatter(evaluated_gens, evaluated_ratings, color='red', label='test run', zorder=5)\n",
    "axes.set_title(f'eval evaluated on benchmark, committee_size: {len(committee_gens)} out of {len(benchmark_gens)}')\n",
    "axes.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83eb3343-c652-4e7b-8441-0237adecbed7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EvalRatingData' object has no attribute 'evaluated_agents'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m committee_gens \u001b[38;5;241m=\u001b[39m [agent\u001b[38;5;241m.\u001b[39mgen \u001b[38;5;28;01mfor\u001b[39;00m i, agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(benchmark_rating_data\u001b[38;5;241m.\u001b[39mcommittee, benchmark_rating_data\u001b[38;5;241m.\u001b[39magents) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m]\n\u001b[1;32m     11\u001b[0m eval_rating_data \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mread_ratings_from_db()\n\u001b[0;32m---> 12\u001b[0m evaluated_gens \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([agent\u001b[38;5;241m.\u001b[39mgen \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[43meval_rating_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluated_agents\u001b[49m])\n\u001b[1;32m     13\u001b[0m evaluated_ratings \u001b[38;5;241m=\u001b[39m eval_rating_data\u001b[38;5;241m.\u001b[39mratings\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EvalRatingData' object has no attribute 'evaluated_agents'"
     ]
    }
   ],
   "source": [
    "game = 'c4'\n",
    "tag = 'benchmark'\n",
    "run_params = RunParams(game, tag)\n",
    "\n",
    "organizer = DirectoryOrganizer(run_params, base_dir_root='/workspace')\n",
    "evaluator = Evaluator(organizer)\n",
    "benchmark_rating_data = evaluator._benchmark.read_ratings_from_db()\n",
    "benchmark_gens = np.array([agent.gen for agent in benchmark_rating_data.agents])\n",
    "benchmark_ratings = benchmark_rating_data.ratings\n",
    "committee_gens = [agent.gen for i, agent in zip(benchmark_rating_data.committee, benchmark_rating_data.agents) if i == True]\n",
    "eval_rating_data = evaluator.read_ratings_from_db()\n",
    "evaluated_gens = np.array([agent.gen for agent in eval_rating_data.evaluated_agents])\n",
    "evaluated_ratings = eval_rating_data.ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a701747d-a1a8-4dd6-a0d3-f76f6bb71c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(16,8))\n",
    "\n",
    "sorted_ix = np.argsort(benchmark_gens)\n",
    "\n",
    "axes.vlines(benchmark_gens, ymin=min(benchmark_ratings), ymax=max(benchmark_ratings)*1.2, color='lightgrey', linestyle='--', label='participated agents')\n",
    "axes.vlines(committee_gens, ymin=min(benchmark_ratings), ymax=max(benchmark_ratings)*1.2, color='orange', linestyle='--', label='committee')\n",
    "axes.plot(benchmark_gens[sorted_ix], benchmark_ratings[sorted_ix], label='elo', zorder=4)\n",
    "\n",
    "axes.scatter(evaluated_gens, evaluated_ratings, color='red', label='test run', zorder=5)\n",
    "axes.set_title(f'benchmark evaluated on eval, committee_size: {len(committee_gens)} out of {len(benchmark_gens)}')\n",
    "axes.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c9c596-050d-48e8-9ed8-17737124e4de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa51b803-4b2c-4319-8770-bfcfb98f29be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0963828b-3e9d-4e8e-8e1a-d2e26c2c8530",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator._benchmark_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883fdd0e-f723-4717-9416-94969d23dab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57e8e46-1297-44f5-84e6-c5f8ba349ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator._arena.ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef560e4-23b0-4458-9b87-ef56457f4506",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_ixs = evaluator.benchmark_agent_ixs()\n",
    "test_ixs = evaluator.test_agent_ixs()\n",
    "evaluator._arena.refresh_ratings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c538f09b-b27b-4230-97f8-32767562a7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = evaluator._arena.ratings[benchmark_ixs]\n",
    "ys = evaluator._benchmark_ratings\n",
    "test_agents_elo = evaluator._arena.ratings[test_ixs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d486c8-e259-45e4-93b2-734d9952c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(16,8))\n",
    "axes.scatter(xs, ys, label='elo', zorder=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357d6ea4-d8d0-4fdf-9659-f9cc15054fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_ixs = np.argsort(xs)\n",
    "xs_sorted = xs[sorted_ixs][evaluator._benchmark_committee]\n",
    "ys_sorted = ys[sorted_ixs][evaluator._benchmark_committee]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e609eb72-8ffb-4aa5-8299-57703a337fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "interp_func = interp1d(xs_sorted, ys_sorted, kind=\"linear\", fill_value=\"extrapolate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97f7ace-c75c-44de-9625-0d0058924660",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_ratings = interp_func(test_agents_elo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c14cee-e0b5-4178-93e5-7e2b028c7646",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be48c21-72c4-4184-8216-2174b6909c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agents_elo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a4e58c-5875-4f4b-970e-2788436fcf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(16,8))\n",
    "axes.scatter(xs_sorted, ys_sorted, label='elo', zorder=4)\n",
    "axes.vlines(test_agents_elo[3], ymin=min(test_agents_elo), ymax=max(test_agents_elo), linestyle='--', color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab02e1-3871-4b65-8602-ea061266e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_sorted[evaluator._benchmark_committee]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86487a7-5ddc-4397-aa70-d7e641282d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_sorted[evaluator._benchmark_committee]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e25ff26-7675-4e62-9138-60e840a08c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator._benchmark_committee"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
